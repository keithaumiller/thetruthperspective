<?php

/**
 * @file
 * News Extractor module - Enhanced architecture with separated services.
 * 
 * ARCHITECTURE:
 * - ScrapingService (Sensors): Handles Diffbot API and content extraction
 * - AIProcessingService (Processors): Handles Claude AI analysis and prompt building
 * - DataProcessingService (Levers): Handles field updates, taxonomy, and formatting
 * - NewsExtractionService (Orchestrator): Coordinates all services in pipeline
 */

use Drupal\Core\Entity\EntityInterface;
use Drupal\Core\Routing\RouteMatchInterface;
use Drupal\node\Entity\Node;

/**
 * Implements hook_help().
 */
function news_extractor_help($route_name, RouteMatchInterface $route_match) {
  switch ($route_name) {
    case 'help.page.news_extractor':
      return '<p>' . t('Enhanced news extraction with separated scraping, AI processing, and data management services.') . '</p>';
  }
}

/**
 * Implements hook_feeds_process_alter().
 * Alter feed item data before it becomes a node.
 */
function news_extractor_feeds_process_alter(&$feed, $item, $entity_interface) {
  \Drupal::logger('news_extractor')->info('Enhanced news_extractor module - feeds_process_alter triggered');

  // Only process Article content type
  if ($entity_interface->getTarget()->bundle() !== 'article') {
    return;
  }

  $title_to_check = $item['title'] ?? '';
  $link_to_check = $item['link'] ?? '';

  // Use scraping service to validate URLs
  /** @var \Drupal\news_extractor\Service\ScrapingService $scraping_service */
  $scraping_service = \Drupal::service('news_extractor.scraping');

  // Skip invalid URLs
  if (!empty($item['link']) && !$scraping_service->isValidArticleUrl($item['link'])) {
    \Drupal::logger('news_extractor')->info('Skipping invalid article URL: @url', [
      '@url' => $item['link'],
    ]);
    $feed->skipItem($item);
    return;
  }

  // Skip items with blocked content or invalid titles
  if (_news_extractor_has_blocked_content($title_to_check, $link_to_check) ||
      _news_extractor_has_invalid_title($title_to_check)) {
    $feed->skipItem($item);
    return;
  }

  // Extract news source from feed data using service
  /** @var \Drupal\news_extractor\Service\DataProcessingService $data_processing_service */
  $data_processing_service = \Drupal::service('news_extractor.data_processing');
  $news_source = $data_processing_service->extractNewsSourceFromFeed($item);
  
  if (!empty($news_source) && $entity_interface->getTarget()->hasField('field_news_source')) {
    $entity_interface->getTarget()->set('field_news_source', $news_source);
    \Drupal::logger('news_extractor')->info('Set news source from feed: @source', [
      '@source' => $news_source,
    ]);
  }
}

/**
 * Implements hook_entity_insert().
 * Process new article entities.
 */
function news_extractor_entity_insert(EntityInterface $entity) {
  if ($entity->bundle() === 'article' && $entity->hasField('field_original_url')) {
    $original_url = $entity->get('field_original_url')->uri ?? '';
    
    if (!empty($original_url)) {
      // Note: News source extraction is handled during Diffbot scraping in ScrapingService::updateMetadataFields()
      // This ensures we have the actual siteName from Diffbot API for accurate source identification
      
      // Use the orchestrator service for complete processing
      /** @var \Drupal\news_extractor\Service\NewsExtractionService $extraction_service */
      $extraction_service = \Drupal::service('news_extractor.extraction');
      $extraction_service->processArticle($entity, $original_url);
    }
  }
}

/**
 * Main extraction function using service architecture.
 * 
 * @param \Drupal\Core\Entity\EntityInterface $entity
 *   The article entity to process.
 * @param string $url
 *   The article URL.
 * 
 * @return bool
 *   TRUE if processing was successful.
 */
function _news_extractor_extract_content(EntityInterface $entity, $url) {
  /** @var \Drupal\news_extractor\Service\NewsExtractionService $extraction_service */
  $extraction_service = \Drupal::service('news_extractor.extraction');
  
  return $extraction_service->processArticle($entity, $url);
}

/**
 * Bulk process articles using the new service architecture.
 * 
 * @param int $limit
 *   Maximum number of articles to process.
 * @param string $processing_type
 *   Type of processing: 'full', 'scrape_only', 'analyze_only', 'reprocess'.
 * 
 * @return array
 *   Processing statistics.
 */
function news_extractor_bulk_process_articles($limit = 20, $processing_type = 'full') {
  /** @var \Drupal\news_extractor\Service\NewsExtractionService $extraction_service */
  $extraction_service = \Drupal::service('news_extractor.extraction');
  
  $stats = [
    'processed' => 0,
    'successful' => 0,
    'failed' => 0,
    'skipped' => 0,
  ];

  // Find articles based on processing type
  $query = \Drupal::entityQuery('node')
    ->condition('type', 'article')
    ->accessCheck(FALSE)
    ->range(0, $limit);

  // Customize query based on processing type
  switch ($processing_type) {
    case 'scrape_only':
      // Articles with URLs but no scraped data
      $query->condition('field_original_url.uri', '', '<>');
      $or_group = $query->orConditionGroup();
      $or_group->condition('field_json_scraped_article_data.value', '', '=');
      $or_group->condition('field_json_scraped_article_data.value', NULL, 'IS NULL');
      $query->condition($or_group);
      break;

    case 'analyze_only':
      // Articles with scraped data but no AI analysis
      $query->condition('field_json_scraped_article_data.value', '', '<>');
      $or_group = $query->orConditionGroup();
      $or_group->condition('field_ai_raw_response.value', '', '=');
      $or_group->condition('field_ai_raw_response.value', NULL, 'IS NULL');
      $query->condition($or_group);
      break;

    case 'reprocess':
      // Articles with AI responses for reprocessing
      $query->condition('field_ai_raw_response.value', '', '<>');
      break;

    case 'full':
    default:
      // Articles with URLs but incomplete processing
      $query->condition('field_original_url.uri', '', '<>');
      break;
  }

  $nids = $query->execute();

  foreach ($nids as $nid) {
    $node = Node::load($nid);
    if (!$node) {
      $stats['skipped']++;
      continue;
    }

    $stats['processed']++;
    $success = FALSE;

    try {
      switch ($processing_type) {
        case 'scrape_only':
          $url = $node->get('field_original_url')->uri;
          $success = $extraction_service->scrapeArticleOnly($node, $url);
          break;

        case 'analyze_only':
          $success = $extraction_service->analyzeArticleOnly($node);
          break;

        case 'reprocess':
          $success = $extraction_service->reprocessArticle($node);
          break;

        case 'full':
        default:
          $url = $node->get('field_original_url')->uri;
          $success = $extraction_service->processArticle($node, $url);
          break;
      }

      if ($success) {
        $stats['successful']++;
      } else {
        $stats['failed']++;
      }

    } catch (\Exception $e) {
      $stats['failed']++;
      \Drupal::logger('news_extractor')->error('Error processing node @nid: @error', [
        '@nid' => $nid,
        '@error' => $e->getMessage(),
      ]);
    }
  }

  \Drupal::logger('news_extractor')->info('Bulk processing (@type) complete: @stats', [
    '@type' => $processing_type,
    '@stats' => json_encode($stats),
  ]);

  return $stats;
}

/**
 * Get processing status for articles.
 * 
 * @param int $limit
 *   Number of articles to check.
 * 
 * @return array
 *   Status information for articles.
 */
function news_extractor_get_processing_status($limit = 10) {
  /** @var \Drupal\news_extractor\Service\NewsExtractionService $extraction_service */
  $extraction_service = \Drupal::service('news_extractor.extraction');
  
  $nids = \Drupal::entityQuery('node')
    ->condition('type', 'article')
    ->accessCheck(FALSE)
    ->sort('created', 'DESC')
    ->range(0, $limit)
    ->execute();

  $status_reports = [];
  foreach ($nids as $nid) {
    $node = Node::load($nid);
    if ($node) {
      $status_reports[] = $extraction_service->getProcessingStatus($node);
    }
  }

  return $status_reports;
}

/**
 * Helper function to check for blocked content in title and link.
 */
function _news_extractor_has_blocked_content($title, $link) {
  $blocked_strings = [
    'comparecards.com',
    'fool.com',
    'lendingtree.com',
  ];

  foreach ($blocked_strings as $str) {
    if (stripos($title, $str) !== FALSE) {
      \Drupal::logger('news_extractor')->info('Skipping blocked string in title: @title', [
        '@title' => $title,
      ]);
      return TRUE;
    }
    
    if (stripos($link, $str) !== FALSE) {
      \Drupal::logger('news_extractor')->info('Skipping blocked string in link: @url', [
        '@url' => $link,
      ]);
      return TRUE;
    }
  }

  return FALSE;
}

/**
 * Helper function to check for invalid titles.
 */
function _news_extractor_has_invalid_title($title) {
  // Skip items with missing or empty titles
  if (empty($title) || trim($title) == '') {
    \Drupal::logger('news_extractor')->info('Skipping item with missing or empty title');
    return TRUE;
  }

  // Skip items with very short titles
  if (strlen(trim($title)) < 10) {
    \Drupal::logger('news_extractor')->info('Skipping item with very short title: @title', [
      '@title' => $title,
    ]);
    return TRUE;
  }

  return FALSE;
}

/**
 * Implements hook_cron().
 * 
 * Automated processing for failed and stale articles:
 * 1. Reprocess articles with "Scraped data unavailable."
 * 2. Unpublish articles that still fail after reprocessing
 * 3. Delete unpublished articles older than 1 day
 */
function news_extractor_cron() {
  $logger = \Drupal::logger('news_extractor');
  $logger->info('Starting news_extractor automated cleanup cron job');

  // Get extraction service
  /** @var \Drupal\news_extractor\Service\NewsExtractionService $extraction_service */
  $extraction_service = \Drupal::service('news_extractor.extraction');

  $stats = [
    'failed_scraping_found' => 0,
    'failed_scraping_fixed' => 0,
    'unpublished_for_failure' => 0,
    'old_articles_deleted' => 0,
  ];

  // STEP 1: Find and reprocess articles with failed scraping
  $logger->info('Step 1: Finding articles with failed scraping data');
  
  $failed_scraping_query = \Drupal::entityQuery('node')
    ->condition('type', 'article')
    ->condition('field_json_scraped_article_data', 'Scraped data unavailable.', '=')
    ->condition('status', 1) // Only published articles
    ->accessCheck(FALSE);
  
  $failed_nids = $failed_scraping_query->execute();
  $stats['failed_scraping_found'] = count($failed_nids);
  
  $logger->info('Found @count articles with failed scraping', ['@count' => $stats['failed_scraping_found']]);

  // Reprocess failed scraping articles (limit to 20 per run to avoid timeouts)
  $reprocess_limit = min(20, count($failed_nids));
  $failed_nodes = \Drupal::entityTypeManager()->getStorage('node')->loadMultiple(array_slice($failed_nids, 0, $reprocess_limit));
  
  foreach ($failed_nodes as $node) {
    try {
      if (!$node->hasField('field_original_url') || $node->get('field_original_url')->isEmpty()) {
        $logger->warning('Node @nid has no URL for reprocessing', ['@nid' => $node->id()]);
        continue;
      }
      
      $url = $node->get('field_original_url')->uri;
      $logger->info('Reprocessing failed article @nid: @title', [
        '@nid' => $node->id(),
        '@title' => $node->getTitle(),
      ]);
      
      // Attempt to reprocess
      $success = $extraction_service->scrapeArticleOnly($node, $url);
      
      if ($success) {
        $stats['failed_scraping_fixed']++;
        $logger->info('Successfully reprocessed article @nid', ['@nid' => $node->id()]);
        
        // Try AI analysis if scraping succeeded
        try {
          $extraction_service->analyzeArticleOnly($node);
        } catch (\Exception $ai_e) {
          $logger->warning('AI analysis failed for reprocessed article @nid: @error', [
            '@nid' => $node->id(),
            '@error' => $ai_e->getMessage(),
          ]);
        }
      } else {
        $logger->warning('Failed to reprocess article @nid', ['@nid' => $node->id()]);
      }
      
    } catch (\Exception $e) {
      $logger->error('Error reprocessing article @nid: @error', [
        '@nid' => $node->id(),
        '@error' => $e->getMessage(),
      ]);
    }
  }

  // STEP 2: Unpublish articles that still have failed processing
  $logger->info('Step 2: Unpublishing articles with persistent processing failures');
  
  // Find articles that still have failed scraping
  $failed_scraping_query = \Drupal::entityQuery('node')
    ->condition('type', 'article')
    ->condition('status', 1) // Only published articles
    ->condition('field_json_scraped_article_data', 'Scraped data unavailable.', '=')
    ->accessCheck(FALSE);
  
  $failed_scraping_nids = $failed_scraping_query->execute();
  
  // Find articles with good scraping but no AI analysis
  $no_ai_query = \Drupal::entityQuery('node')
    ->condition('type', 'article')
    ->condition('status', 1) // Only published articles
    ->condition('field_json_scraped_article_data', 'Scraped data unavailable.', '<>')
    ->condition('field_json_scraped_article_data', '', '<>')
    ->accessCheck(FALSE);
  
  $ai_or_group = $no_ai_query->orConditionGroup();
  $ai_or_group->condition('field_ai_raw_response', NULL, 'IS NULL');
  $ai_or_group->condition('field_ai_raw_response', '', '=');
  $no_ai_query->condition($ai_or_group);
  
  $no_ai_nids = $no_ai_query->execute();
  
  // Combine both sets of problematic articles
  $still_failed_nids = array_unique(array_merge($failed_scraping_nids, $no_ai_nids));
  
  if (!empty($still_failed_nids)) {
    $still_failed_nodes = \Drupal::entityTypeManager()->getStorage('node')->loadMultiple($still_failed_nids);
    
    foreach ($still_failed_nodes as $node) {
      // Unpublish the node
      $node->set('status', 0);
      $node->save();
      $stats['unpublished_for_failure']++;
      
      $logger->warning('Unpublished article with pending motivation analysis: @title (ID: @nid)', [
        '@nid' => $node->id(),
        '@title' => $node->getTitle(),
      ]);
    }
  }

  // STEP 3: Delete old unpublished articles with processing failures
  $logger->info('Step 3: Deleting old unpublished articles with processing failures');
  
  // Find unpublished articles older than 24 hours with processing issues
  $one_day_ago = \Drupal::time()->getRequestTime() - (24 * 60 * 60);
  
  // Find old articles with failed scraping
  $old_failed_scraping_query = \Drupal::entityQuery('node')
    ->condition('type', 'article')
    ->condition('status', 0) // Only unpublished articles
    ->condition('created', $one_day_ago, '<') // Older than 24 hours
    ->condition('field_json_scraped_article_data', 'Scraped data unavailable.', '=')
    ->accessCheck(FALSE);
  
  $old_failed_scraping_nids = $old_failed_scraping_query->execute();
  
  // Find old articles with good scraping but no AI analysis
  $old_no_ai_query = \Drupal::entityQuery('node')
    ->condition('type', 'article')
    ->condition('status', 0) // Only unpublished articles
    ->condition('created', $one_day_ago, '<') // Older than 24 hours
    ->condition('field_json_scraped_article_data', 'Scraped data unavailable.', '<>')
    ->condition('field_json_scraped_article_data', '', '<>')
    ->accessCheck(FALSE);
  
  $old_ai_or_group = $old_no_ai_query->orConditionGroup();
  $old_ai_or_group->condition('field_ai_raw_response', NULL, 'IS NULL');
  $old_ai_or_group->condition('field_ai_raw_response', '', '=');
  $old_no_ai_query->condition($old_ai_or_group);
  
  $old_no_ai_nids = $old_no_ai_query->execute();
  
  // Combine both sets of old problematic articles
  $old_failed_nids = array_unique(array_merge($old_failed_scraping_nids, $old_no_ai_nids));
  
  if (!empty($old_failed_nids)) {
    // Limit deletions to prevent timeouts (max 50 per run)
    $delete_limit = min(50, count($old_failed_nids));
    $old_failed_nodes = \Drupal::entityTypeManager()->getStorage('node')->loadMultiple(array_slice($old_failed_nids, 0, $delete_limit));
    
    foreach ($old_failed_nodes as $node) {
      $title = $node->getTitle();
      $nid = $node->id();
      
      try {
        $node->delete();
        $stats['old_articles_deleted']++;
        
        $logger->info('Deleted old failed article @nid: @title', [
          '@nid' => $nid,
          '@title' => $title,
        ]);
      } catch (\Exception $e) {
        $logger->error('Error deleting old article @nid: @error', [
          '@nid' => $nid,
          '@error' => $e->getMessage(),
        ]);
      }
    }
  }

  // Log final statistics
  $logger->info('News extractor cron job completed. Statistics: @stats', [
    '@stats' => json_encode($stats),
  ]);

  // Also log human-readable summary
  $logger->info('Cron summary: Found @found failed articles, fixed @fixed, unpublished @unpublished persistent failures, deleted @deleted old articles', [
    '@found' => $stats['failed_scraping_found'],
    '@fixed' => $stats['failed_scraping_fixed'],
    '@unpublished' => $stats['unpublished_for_failure'],
    '@deleted' => $stats['old_articles_deleted'],
  ]);
}
